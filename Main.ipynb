{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21e1a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "from io import StringIO\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4e18e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données importées depuis ./data/data/sp_data.csv et ./data/data/sp_indice_data.csv\n",
      "Données d'entrainement importé depuis ./data/training_data/training_sp.csv, ./data/training_data/training_indice_sp.csv et ./data/training_data/training_indice_sp_vol.csv\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "if not os.path.exists(\"data\"):    \n",
    "    os.makedirs(\"data\")\n",
    "if not os.path.exists(\"data/history\"):           \n",
    "    os.makedirs(\"data/history\")\n",
    "if not os.path.exists(\"data/data\"):   \n",
    "    os.makedirs(\"data/data\")\n",
    "if not os.path.exists(\"data/training_data\"):\n",
    "    os.makedirs(\"data/training_data\")\n",
    "if not os.path.exists(\"data/model\"):\n",
    "    os.makedirs(\"data/model\")\n",
    "if not os.path.exists(\"data/training_data_x_classifier\"):\n",
    "    os.makedirs(\"data/training_data_x_classifier\")\n",
    "    \n",
    "if not os.path.exists(\"data/training_data/training_sp.csv\"):\n",
    "    tickers = [\"AAPL\", \"MSFT\", \"NVDA\",\"^TNX\"]\n",
    "    training_sp = yf.download(tickers,start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_indice_sp = yf.download(\"^GSPC\",start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_indice_sp_vol = yf.download(\"^VIX\",start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_sp.to_csv(\"data/training_data/training_sp.csv\")\n",
    "    training_indice_sp.to_csv(\"data/training_data/training_indice_sp.csv\")\n",
    "    training_indice_sp_vol.to_csv(\"data/training_data/training_indice_sp_vol.csv\")\n",
    "if not os.path.exists(\"data/history/sp500.txt\"):\n",
    "    with open(\"data/history/sp500.txt\",\"w\") as sp:\n",
    "        initialisation = \"2000-01-01 00:00:00\"\n",
    "        initialisation = np.datetime64(initialisation,'s')\n",
    "        initialisation = str(initialisation.astype(\"int64\"))\n",
    "        sp.write(initialisation)\n",
    "with open(\"data/history/sp500.txt\",\"r\") as sp:\n",
    "    row = sp.readlines()\n",
    "    last_line = int(row[-1].strip())\n",
    "if last_line//86400 != start_time//86400:                 \n",
    "    url = \"https://stockanalysis.com/list/sp-500-stocks/\" \n",
    "    headers = {\"user-agent\":\"Mozilla/5.0\"}                \n",
    "    reponse = requests.get(url, headers=headers)\n",
    "    tickers = pd.read_html(StringIO(reponse.text))\n",
    "    tickers = tickers[0][\"Symbol\"]\n",
    "    tickers = tickers.to_list()\n",
    "    if \"GOOG\" in tickers:                                     \n",
    "        tickers.remove(\"GOOG\") \n",
    "    tickers = tickers[0:3]                \n",
    "    tickers = [ticker.replace(\".\",\"-\") for ticker in tickers] \n",
    "    sp_data = yf.download(tickers,period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    sp_indice_data = yf.download(\"^GSPC\", period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    sp_indice_vol_data = yf.download(\"^VIX\",period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    sp_data.to_csv(\"data/data/sp_data.csv\")                   \n",
    "    sp_indice_data.to_csv(\"data/data/sp_indice_data.csv\")    \n",
    "    sp_indice_vol_data.to_csv(\"data/data/sp_indice_vol_data.csv\") \n",
    "    with open(\"data/history/sp500.txt\",\"a\") as sp:\n",
    "        note = str(np.int64(start_time))\n",
    "        sp.write(f\"\\n{note}\")\n",
    "    print(\"Données actualisées depuis Yfinance\")\n",
    "else:                                                             \n",
    "    sp_data = pd.read_csv(\"data/data/sp_data.csv\",index_col=[0],header=[0,1])                \n",
    "    sp_indice_data = pd.read_csv(\"data/data/sp_indice_data.csv\",index_col=[0],header=[0,1])\n",
    "    sp_indice_vol_data = pd.read_csv(\"data/data/sp_indice_vol_data.csv\",index_col=[0],header=[0,1])                                  \n",
    "    sp_data.index = pd.to_datetime(sp_data.index)                 \n",
    "    sp_indice_data.index = pd.to_datetime(sp_indice_data.index)\n",
    "    sp_indice_vol_data.index = pd.to_datetime(sp_indice_vol_data.index)\n",
    "    print(\"Données importées depuis ./data/data/sp_data.csv et ./data/data/sp_indice_data.csv\")\n",
    "training_sp = pd.read_csv(\"data/training_data/training_sp.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_sp = pd.read_csv(\"data/training_data/training_indice_sp.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_sp_vol = pd.read_csv(\"data/training_data/training_indice_sp_vol.csv\",index_col=[0],header=[0,1])\n",
    "training_sp.index = pd.to_datetime(training_sp.index)\n",
    "training_indice_sp.index = pd.to_datetime(training_indice_sp.index)\n",
    "training_indice_sp_vol.index = pd.to_datetime(training_indice_sp_vol.index)\n",
    "print(\"Données d'entrainement importé depuis ./data/training_data/training_sp.csv,\"\n",
    "\" ./data/training_data/training_indice_sp.csv et ./data/training_data/training_indice_sp_vol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9664b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données importées depuis ./data/data/cac_data.csv et ./data/data/cac_indice_data.csv\n",
      "Données d'entrainement importé depuis ./data/training_data/training_cac.csv et ./data/training_data/training_indice_cac.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"data/training_data/training_cac.csv\"):\n",
    "    tickers = [\"MC.PA\",\"TTE.PA\",\"SAN.PA\"]                \n",
    "    training_cac = yf.download(tickers,start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_indice_cac = yf.download(\"^FCHI\",start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_cac.to_csv(\"data/training_data/training_cac.csv\")\n",
    "    training_indice_cac.to_csv(\"data/training_data/training_indice_cac.csv\")\n",
    "if not os.path.exists(\"data/history/cac40.txt\"):\n",
    "    with open(\"data/history/cac40.txt\",\"w\") as cac:\n",
    "        initialisation = \"2000-01-01 00:00:00\"\n",
    "        initialisation = np.datetime64(initialisation,'s')\n",
    "        initialisation = str(initialisation.astype(\"int64\"))\n",
    "        cac.write(initialisation)\n",
    "with open(\"data/history/cac40.txt\",\"r\") as cac:\n",
    "    row = cac.readlines()\n",
    "    last_line = int(row[-1].strip())\n",
    "if last_line//86400 != start_time//86400:                  \n",
    "    url = \"https://fr.finance.yahoo.com/quote/%5EFCHI/components/\" \n",
    "    headers = {\"user-agent\":\"Mozilla/5.0\"}                 \n",
    "    reponse = requests.get(url, headers=headers)\n",
    "    tickers = pd.read_html(StringIO(reponse.text))\n",
    "    tickers = tickers[0][\"Symbole\"]\n",
    "    tickers = tickers[0:10]\n",
    "    tickers = tickers.to_list()\n",
    "    cac_data = yf.download(tickers,period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    cac_indice_data = yf.download(\"^FCHI\", period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    cac_data.to_csv(\"data/data/cac_data.csv\")                   \n",
    "    cac_indice_data.to_csv(\"data/data/cac_indice_data.csv\")     \n",
    "    with open(\"data/history/cac40.txt\",\"a\") as cac:\n",
    "        note = str(np.int64(start_time))\n",
    "        cac.write(f\"\\n{note}\")\n",
    "    print(\"Données actualisées depuis Yfinance\")\n",
    "else:                                                              \n",
    "    cac_data = pd.read_csv(                                        \n",
    "        \"data/data/cac_data.csv\",                                  \n",
    "        index_col=[0],header=[0,1]\n",
    "        )                \n",
    "    cac_indice_data = pd.read_csv(\"data/data/cac_indice_data.csv\",index_col=[0],header=[0,1])                                  \n",
    "    cac_data.index = pd.to_datetime(cac_data.index)                 \n",
    "    cac_indice_data.index = pd.to_datetime(cac_indice_data.index)\n",
    "    print(\"Données importées depuis ./data/data/cac_data.csv et ./data/data/cac_indice_data.csv\")\n",
    "training_cac = pd.read_csv(\"data/training_data/training_cac.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_cac = pd.read_csv(\"data/training_data/training_indice_cac.csv\",index_col=[0],header=[0,1])\n",
    "training_cac.index, training_indice_cac.index = pd.to_datetime(training_cac.index), pd.to_datetime(training_indice_cac.index)\n",
    "print(\"Données d'entrainement importé depuis ./data/training_data/training_cac.csv et ./data/training_data/training_indice_cac.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea9b1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données importées depuis ./data/data/ftse_data.csv et ./data/data/ftse_indice_data.csv\n",
      "Données d'entrainement importé depuis ./data/training_data/training_ftse.csv et ./data/training_data/training_indice_ftse.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"data/training_data/training_ftse.csv\"):\n",
    "    tickers = [\"AZN.L\",\"HSBA.L\",\"ULVR.L\"]                \n",
    "    training_ftse = yf.download(tickers,start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_indice_ftse = yf.download(\"^FTSE\",start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_ftse.to_csv(\"data/training_data/training_ftse.csv\")\n",
    "    training_indice_ftse.to_csv(\"data/training_data/training_indice_ftse.csv\")\n",
    "if not os.path.exists(\"data/history/ftse100.txt\"):\n",
    "    with open(\"data/history/ftse100.txt\",\"w\") as ftse:\n",
    "        initialisation = \"2000-01-01 00:00:00\"\n",
    "        initialisation = np.datetime64(initialisation,'s')\n",
    "        initialisation = str(initialisation.astype(\"int64\"))\n",
    "        ftse.write(initialisation)\n",
    "with open(\"data/history/ftse100.txt\",\"r\") as ftse:\n",
    "    row = ftse.readlines()\n",
    "    last_line = int(row[-1].strip())\n",
    "if last_line//86400 != start_time//86400:                                          \n",
    "    url = \"https://uk.finance.yahoo.com/quote/%5EFTSE/components/\" \n",
    "    headers = {\"user-agent\":\"Mozilla/5.0\"}                                         \n",
    "    reponse = requests.get(url, headers=headers)\n",
    "    tickers = pd.read_html(StringIO(reponse.text))\n",
    "    tickers = tickers[0][\"Symbol\"]\n",
    "    tickers = tickers.to_list()\n",
    "    tickers = tickers[0:10]\n",
    "    ftse_data = yf.download(tickers,period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    ftse_indice_data = yf.download(\"^FTSE\", period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    ftse_data.to_csv(\"data/data/ftse_data.csv\")                   \n",
    "    ftse_indice_data.to_csv(\"data/data/ftse_indice_data.csv\")     \n",
    "    with open(\"data/history/ftse100.txt\",\"a\") as ftse:\n",
    "        note = str(np.int64(start_time))\n",
    "        ftse.write(f\"\\n{note}\")\n",
    "    print(\"Données actualisées depuis Yfinance\")\n",
    "else:                                                              \n",
    "    ftse_data = pd.read_csv(                                        \n",
    "        \"data/data/ftse_data.csv\",                                  \n",
    "        index_col=[0],header=[0,1]\n",
    "        )                \n",
    "    ftse_indice_data = pd.read_csv(\"data/data/ftse_indice_data.csv\",index_col=[0],header=[0,1])                                  \n",
    "    ftse_data.index = pd.to_datetime(ftse_data.index)                 \n",
    "    ftse_indice_data.index = pd.to_datetime(ftse_indice_data.index)\n",
    "    print(\"Données importées depuis ./data/data/ftse_data.csv et ./data/data/ftse_indice_data.csv\")\n",
    "training_ftse = pd.read_csv(\"data/training_data/training_ftse.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_ftse = pd.read_csv(\"data/training_data/training_indice_ftse.csv\",index_col=[0],header=[0,1])\n",
    "training_ftse.index, training_indice_ftse.index = pd.to_datetime(training_ftse.index), pd.to_datetime(training_indice_ftse.index)\n",
    "print(\"Données d'entrainement importé depuis ./data/training_data/training_ftse.csv et ./data/training_data/training_indice_ftse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6878798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données importées depuis ./data/data/dax_data.csv et ./data/data/dax_indice_data.csv\n",
      "Données d'entrainement importé depuis ./data/training_data/training_dax.csv et ./data/training_data/training_indice_dax.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"data/training_data/training_dax.csv\"):\n",
    "    tickers = [\"SAP.DE\",\"SIE.DE\",\"ALV.DE\"]              \n",
    "    training_dax = yf.download(tickers,start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_indice_dax = yf.download(\"^GDAXI\",start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_dax.to_csv(\"data/training_data/training_dax.csv\")\n",
    "    training_indice_dax.to_csv(\"data/training_data/training_indice_dax.csv\")\n",
    "if not os.path.exists(\"data/history/dax40.txt\"):\n",
    "    with open(\"data/history/dax40.txt\",\"w\") as dax:\n",
    "        initialisation = \"2000-01-01 00:00:00\"\n",
    "        initialisation = np.datetime64(initialisation,'s')\n",
    "        initialisation = str(initialisation.astype(\"int64\"))\n",
    "        dax.write(initialisation)\n",
    "with open(\"data/history/dax40.txt\",\"r\") as dax:\n",
    "    row = dax.readlines()\n",
    "    last_line = int(row[-1].strip())\n",
    "if last_line//86400 != start_time//86400:                                          \n",
    "    url = \"https://finance.yahoo.com/quote/%5EGDAXI/components/\" \n",
    "    headers = {\"user-agent\":\"Mozilla/5.0\"}                                         \n",
    "    reponse = requests.get(url, headers=headers)\n",
    "    tickers = pd.read_html(StringIO(reponse.text))\n",
    "    tickers = tickers[0][\"Symbol\"]\n",
    "    tickers = tickers.to_list()\n",
    "    tickers = tickers[0:10]\n",
    "    dax_data = yf.download(tickers,period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    dax_indice_data = yf.download(\"^GDAXI\", period=\"5d\",interval=\"1d\")[[\"Open\",\"Close\",\"Volume\"]]\n",
    "    dax_data.to_csv(\"data/data/dax_data.csv\")                   \n",
    "    dax_indice_data.to_csv(\"data/data/dax_indice_data.csv\")     \n",
    "    with open(\"data/history/dax40.txt\",\"a\") as dax:\n",
    "        note = str(np.int64(start_time))\n",
    "        dax.write(f\"\\n{note}\")\n",
    "    print(\"Données actualisées depuis Yfinance\")\n",
    "else:                                                              \n",
    "    dax_data = pd.read_csv(                                        \n",
    "        \"data/data/dax_data.csv\",                                  \n",
    "        index_col=[0],header=[0,1]\n",
    "        )                \n",
    "    dax_indice_data = pd.read_csv(\"data/data/dax_indice_data.csv\",index_col=[0],header=[0,1])                                  \n",
    "    dax_data.index = pd.to_datetime(dax_data.index)                 \n",
    "    dax_indice_data.index = pd.to_datetime(dax_indice_data.index)\n",
    "    print(\"Données importées depuis ./data/data/dax_data.csv et ./data/data/dax_indice_data.csv\")\n",
    "training_dax = pd.read_csv(\"data/training_data/training_dax.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_dax = pd.read_csv(\"data/training_data/training_indice_dax.csv\",index_col=[0],header=[0,1])\n",
    "training_dax.index, training_indice_dax.index = pd.to_datetime(training_dax.index), pd.to_datetime(training_indice_dax.index)\n",
    "print(\"Données d'entrainement importé depuis ./data/training_data/training_dax.csv et ./data/training_data/training_indice_dax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "482c360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = sorted(set(training_sp.columns.get_level_values(1)))\n",
    "for ticker in tickers:\n",
    "    training_sp[(\"log_return\",ticker)] = np.log(training_sp[(\"Close\",ticker)]/\n",
    "                                                training_sp[(\"Open\",ticker)])\n",
    "for ticker in tickers: \n",
    "    if ticker != \"^TNX\":\n",
    "        training_sp[(\"open_gap_up\",ticker)] = np.where(\n",
    "            training_sp[(\"Open\",ticker)].shift(-1)>training_sp[(\"Close\",ticker)]*1.01,1,0\n",
    "            )\n",
    "for ticker in tickers:\n",
    "    if ticker != \"^TNX\":\n",
    "        training_sp[(\"open_gap_down\",ticker)] = np.where(\n",
    "            training_sp[(\"Open\",ticker)].shift(-1)<training_sp[(\"Close\",ticker)]*0.99,1,0\n",
    "            )\n",
    "training_sp = training_sp.iloc[:,-10:]\n",
    "\n",
    "training_indice_sp_vol[(\"log_return\",\"^VIX\")] = np.log(training_indice_sp_vol[(\"Close\",\"^VIX\")]/\n",
    "                                                training_indice_sp_vol[(\"Open\",\"^VIX\")])\n",
    "training_indice_sp_vol = training_indice_sp_vol.iloc[:,-1]\n",
    "\n",
    "x_classifier_sp = training_sp.merge(training_indice_sp_vol,how=\"inner\",left_index=True,right_index=True)\n",
    "previous_indice_day = {}\n",
    "for i in range(0,5):\n",
    "    previous_indice_day[(\"shift\",f\"shift {str(i)}\")] = np.log(\n",
    "        training_indice_sp[(\"Close\",\"^GSPC\")].shift(i)/training_indice_sp[(\"Open\",\"^GSPC\")].shift(i)\n",
    "        )\n",
    "previous_indice_day[(\"gap\",\"open_gap_up\")] = np.where(\n",
    "    training_indice_sp[(\"Open\",\"^GSPC\")].shift(-1)>training_indice_sp[(\"Close\",\"^GSPC\")]*1.01,1,0\n",
    "    )\n",
    "previous_indice_day[(\"gap\",\"open_gap_down\")] = np.where(\n",
    "    training_indice_sp[(\"Open\",\"^GSPC\")].shift(-1)<training_indice_sp[(\"Close\",\"^GSPC\")]*0.99,1,0\n",
    "    )\n",
    "previous_indice_day  = pd.DataFrame(previous_indice_day, index=training_indice_sp.index)\n",
    "x_classifier_sp_csv = x_classifier_sp.merge(previous_indice_day,how=\"inner\",left_index=True,right_index=True)\n",
    "x_classifier_sp_csv.to_csv(\"data/training_data_x_classifier/x_classifier_sp.csv\")\n",
    "previous_indice_day = previous_indice_day.iloc[i:-2,:]\n",
    "x_classifier_sp = x_classifier_sp.iloc[i:-2,:]\n",
    "x_classifier_sp = x_classifier_sp.to_numpy()\n",
    "previous_indice_day = previous_indice_day.to_numpy()\n",
    "x_classifier_sp = np.concatenate((x_classifier_sp,previous_indice_day), axis=1)\n",
    "y_classifier_sp = np.where(training_indice_sp[(\"Close\", \"^GSPC\")].shift(-1)>\n",
    "training_indice_sp[(\"Open\", \"^GSPC\")].shift(-1),1,0)\n",
    "y_classifier_sp = y_classifier_sp[i:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93b7a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sp = True\n",
    "if trained_sp == False:\n",
    "    class_weight_dict = {   \"standard\":None,\n",
    "                            \"balanced\":\"balanced\",\n",
    "                            \"signal\":{0:2.0,1:1.0}  }\n",
    "    classifier_model_sp = {}\n",
    "    model_accuracy_sp = {}\n",
    "    classification_report_sp = {}\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_classifier_sp, y_classifier_sp, test_size=0.3, shuffle=False)\n",
    "    for name,weight in class_weight_dict.items():\n",
    "        model = RandomForestClassifier( n_estimators=4000,\n",
    "                                        max_depth=10,\n",
    "                                        min_samples_split=10,\n",
    "                                        min_samples_leaf=4,\n",
    "                                        max_features='sqrt',\n",
    "                                        class_weight=weight,\n",
    "                                        n_jobs=-1   )\n",
    "        model = model.fit(x_train,y_train)\n",
    "        classifier_model_sp[name] = model\n",
    "        y_pred = classifier_model_sp[name].predict(x_test)\n",
    "        model_accuracy_sp[name] = accuracy_score(y_test, y_pred)\n",
    "        print(f\"La précision du modèle {name} est de {round(model_accuracy_sp[name]*100,3)} %\")\n",
    "        classification_report_sp[name] = classification_report(y_test, y_pred)\n",
    "        print(classification_report_sp[name])\n",
    "    joblib.dump(classifier_model_sp, \"data/model/classifier_model_sp.joblib\")\n",
    "    joblib.dump(model_accuracy_sp, \"data/model/model_accuracy_sp.joblib\")\n",
    "    joblib.dump(classification_report_sp, \"data/model/classification_report_sp.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "34734b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indice_sp_vol = pd.read_csv( \n",
    "    \"data/training_data/training_indice_sp_vol.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_sp_vol.index = pd.to_datetime(training_indice_sp_vol.index)\n",
    "\n",
    "training_cac, training_indice_sp_vol = training_cac.align(training_indice_sp_vol, join=\"inner\", axis=0)\n",
    "training_cac, training_indice_cac = training_cac.align(training_indice_cac, join=\"inner\",axis=0)\n",
    "\n",
    "tickers = sorted(set(training_cac.columns.get_level_values(1)))\n",
    "for ticker in tickers:\n",
    "    training_cac[(\"log_return\",ticker)] = np.log(training_cac[(\"Close\",ticker)]/\n",
    "                                                training_cac[(\"Open\",ticker)])\n",
    "for ticker in tickers:\n",
    "    training_cac[(\"open_gap_up\",ticker)] = np.where(\n",
    "        training_cac[(\"Open\",ticker)].shift(-1)>training_cac[(\"Close\",ticker)]*1.01,1,0\n",
    "        )\n",
    "for ticker in tickers:\n",
    "    training_cac[(\"open_gap_down\",ticker)] = np.where(\n",
    "        training_cac[(\"Open\",ticker)].shift(-1)<training_cac[(\"Close\",ticker)]*0.99,1,0\n",
    "        )\n",
    "training_cac = training_cac.iloc[:,-9:]\n",
    "\n",
    "training_indice_sp_vol[(\"log_return\",\"^VIX\")] = np.log(training_indice_sp_vol[(\"Close\",\"^VIX\")]/\n",
    "                                                training_indice_sp_vol[(\"Open\",\"^VIX\")])\n",
    "training_indice_sp_vol = training_indice_sp_vol.iloc[:,-1]\n",
    "\n",
    "x_classifier_cac = training_cac.merge(training_indice_sp_vol,how=\"inner\",left_index=True,right_index=True)\n",
    "previous_indice_day = {}\n",
    "for i in range(0,5):\n",
    "    previous_indice_day[(\"shift\",f\"shift {str(i)}\")] = np.log(\n",
    "        training_indice_cac[(\"Close\",\"^FCHI\")].shift(i)/training_indice_cac[(\"Open\",\"^FCHI\")].shift(i)\n",
    "        )\n",
    "previous_indice_day[(\"gap\",\"open_gap_up\")] = np.where(\n",
    "    training_indice_cac[(\"Open\",\"^FCHI\")].shift(-1)>training_indice_cac[(\"Close\",\"^FCHI\")]*1.01,1,0\n",
    "    )\n",
    "previous_indice_day[(\"gap\",\"open_gap_down\")] = np.where(\n",
    "    training_indice_cac[(\"Open\",\"^FCHI\")].shift(-1)<training_indice_cac[(\"Close\",\"^FCHI\")]*0.99,1,0\n",
    "    )\n",
    "previous_indice_day = pd.DataFrame(previous_indice_day, index=training_indice_cac.index)\n",
    "x_classifier_cac_csv = x_classifier_cac.merge(previous_indice_day,how=\"inner\",left_index=True,right_index=True)\n",
    "x_classifier_cac_csv.to_csv(\"data/training_data_x_classifier/x_classifier_cac.csv\")\n",
    "previous_indice_day = previous_indice_day.iloc[i:-2,:]\n",
    "x_classifier_cac = x_classifier_cac.iloc[i:-2,:]\n",
    "x_classifier_cac = x_classifier_cac.to_numpy()\n",
    "previous_indice_day = previous_indice_day.to_numpy()\n",
    "x_classifier_cac = np.concatenate((x_classifier_cac,previous_indice_day), axis=1)\n",
    "y_classifier_cac = np.where(training_indice_cac[(\"Close\", \"^FCHI\")].shift(-1)>\n",
    "training_indice_cac[(\"Open\", \"^FCHI\")].shift(-1),1,0)\n",
    "y_classifier_cac = y_classifier_cac[i:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e338d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_cac =   True\n",
    "if trained_cac == False:\n",
    "    class_weight_dict = {   \"standard\":None,\n",
    "                            \"balanced\":\"balanced\",\n",
    "                            \"signal\":{0:1.2,1:1.0}  }\n",
    "    classifier_model_cac = {}\n",
    "    model_accuracy_cac = {}\n",
    "    classification_report_cac = {}\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_classifier_cac, y_classifier_cac, test_size=0.1, shuffle=False)\n",
    "    for name,weight in class_weight_dict.items():\n",
    "        model = RandomForestClassifier( n_estimators=4000,\n",
    "                                        max_depth=10,\n",
    "                                        min_samples_split=10,\n",
    "                                        min_samples_leaf=4,\n",
    "                                        max_features='sqrt',\n",
    "                                        class_weight=weight,\n",
    "                                        n_jobs=-1   )\n",
    "        model = model.fit(x_train,y_train)\n",
    "        classifier_model_cac[name] = model\n",
    "        y_pred = classifier_model_cac[name].predict(x_test)\n",
    "        model_accuracy_cac[name] = accuracy_score(y_test, y_pred)\n",
    "        print(f\"La précision du modèle {name} est de {round(model_accuracy_cac[name]*100,3)} %\")\n",
    "        classification_report_cac[name] = classification_report(y_test, y_pred)\n",
    "        print(classification_report_cac[name])\n",
    "    joblib.dump(classifier_model_cac, \"data/model/classifier_model_cac.joblib\")\n",
    "    joblib.dump(model_accuracy_cac, \"data/model/model_accuracy_cac.joblib\")\n",
    "    joblib.dump(classification_report_cac, \"data/model/classification_report_cac.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6991e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indice_sp_vol = pd.read_csv( \n",
    "    \"data/training_data/training_indice_sp_vol.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_sp_vol.index = pd.to_datetime(training_indice_sp_vol.index)\n",
    "\n",
    "training_dax, training_indice_sp_vol = training_dax.align(training_indice_sp_vol, join=\"inner\", axis=0)\n",
    "training_dax, training_indice_dax = training_dax.align(training_indice_dax, join=\"inner\",axis=0)\n",
    "\n",
    "tickers = sorted(set(training_dax.columns.get_level_values(1)))\n",
    "for ticker in tickers:\n",
    "    training_dax[(\"log_return\",ticker)] = np.log(training_dax[(\"Close\",ticker)]/\n",
    "                                                training_dax[(\"Open\",ticker)])\n",
    "for ticker in tickers:\n",
    "    training_dax[(\"open_gap_up\",ticker)] = np.where(\n",
    "        training_dax[(\"Open\",ticker)].shift(-1)>training_dax[(\"Close\",ticker)]*1.01,1,0\n",
    "        )\n",
    "for ticker in tickers:\n",
    "    training_dax[(\"open_gap_down\",ticker)] = np.where(\n",
    "        training_dax[(\"Open\",ticker)].shift(-1)<training_dax[(\"Close\",ticker)]*0.99,1,0\n",
    "        )\n",
    "training_dax = training_dax.iloc[:,-9:]\n",
    "\n",
    "training_indice_sp_vol[(\"log_return\",\"^VIX\")] = np.log(training_indice_sp_vol[(\"Close\",\"^VIX\")]/\n",
    "                                                training_indice_sp_vol[(\"Open\",\"^VIX\")])\n",
    "training_indice_sp_vol = training_indice_sp_vol.iloc[:,-1]\n",
    "\n",
    "x_classifier_dax = training_dax.merge(training_indice_sp_vol,how=\"inner\",left_index=True,right_index=True)\n",
    "previous_indice_day = {}\n",
    "for i in range(0,5):\n",
    "    previous_indice_day[(\"shift\",f\"shift {str(i)}\")] = np.log(\n",
    "        training_indice_dax[(\"Close\",\"^GDAXI\")].shift(i)/training_indice_dax[(\"Open\",\"^GDAXI\")].shift(i)\n",
    "        )\n",
    "previous_indice_day[(\"gap\",\"open_gap_up\")] = np.where(\n",
    "    training_indice_dax[(\"Open\",\"^GDAXI\")].shift(-1)>training_indice_dax[(\"Close\",\"^GDAXI\")]*1.01,1,0\n",
    "    )\n",
    "previous_indice_day[(\"gap\",\"open_gap_down\")] = np.where(\n",
    "    training_indice_dax[(\"Open\",\"^GDAXI\")].shift(-1)<training_indice_dax[(\"Close\",\"^GDAXI\")]*0.99,1,0\n",
    "    )\n",
    "previous_indice_day = pd.DataFrame(previous_indice_day, index=training_indice_dax.index)\n",
    "x_classifier_dax_csv = x_classifier_dax.merge(previous_indice_day,how=\"inner\",left_index=True,right_index=True)\n",
    "x_classifier_dax_csv.to_csv(\"data/training_data_x_classifier/x_classifier_dax.csv\")\n",
    "previous_indice_day = previous_indice_day.iloc[i:-2,:]\n",
    "x_classifier_dax = x_classifier_dax.iloc[i:-2,:]\n",
    "x_classifier_dax = x_classifier_dax.to_numpy()\n",
    "previous_indice_day = previous_indice_day.to_numpy()\n",
    "x_classifier_dax = np.concatenate((x_classifier_dax,previous_indice_day), axis=1)\n",
    "y_classifier_dax = np.where(training_indice_dax[(\"Close\", \"^GDAXI\")].shift(-1)>\n",
    "training_indice_dax[(\"Open\", \"^GDAXI\")].shift(-1),1,0)\n",
    "y_classifier_dax = y_classifier_dax[i:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a9c0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_dax = True\n",
    "if trained_dax == False:\n",
    "    class_weight_dict = {   \"standard\":None,\n",
    "                            \"balanced\":\"balanced\",\n",
    "                            \"signal\":{0:1.3,1:1.0}  }\n",
    "    classifier_model_dax = {}\n",
    "    model_accuracy_dax = {}\n",
    "    classification_report_dax = {}\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_classifier_dax, y_classifier_dax, test_size=0.1, shuffle=False)\n",
    "    for name,weight in class_weight_dict.items():\n",
    "        model = RandomForestClassifier( n_estimators=4000,\n",
    "                                        max_depth=10,\n",
    "                                        min_samples_split=10,\n",
    "                                        min_samples_leaf=4,\n",
    "                                        max_features='sqrt',\n",
    "                                        class_weight=weight,\n",
    "                                        n_jobs=-1   )\n",
    "        model = model.fit(x_train,y_train)\n",
    "        classifier_model_dax[name] = model\n",
    "        y_pred = classifier_model_dax[name].predict(x_test)\n",
    "        model_accuracy_dax[name] = accuracy_score(y_test, y_pred)\n",
    "        print(f\"La précision du modèle {name} est de {round(model_accuracy_dax[name]*100,3)} %\")\n",
    "        classification_report_dax[name] = classification_report(y_test, y_pred)\n",
    "        print(classification_report_dax[name])\n",
    "    joblib.dump(classifier_model_dax, \"data/model/classifier_model_dax.joblib\")\n",
    "    joblib.dump(model_accuracy_dax, \"data/model/model_accuracy_dax.joblib\")\n",
    "    joblib.dump(classification_report_dax, \"data/model/classification_report_dax.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0f4196cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/training_data/training_indice_sp_vol.csv\"):\n",
    "    training_indice_sp_vol = yf.download(\"^VIX\",start=\"2001-01-01\",end=\"2024-12-31\",interval=\"1d\")[[\"Open\",\"Close\"]]\n",
    "    training_indice_sp_vol.to_csv(\"data/training_data/training_indice_sp_vol.csv\")\n",
    "training_indice_sp_vol = pd.read_csv( \n",
    "    \"data/training_data/training_indice_sp_vol.csv\",index_col=[0],header=[0,1])\n",
    "training_indice_sp_vol.index = pd.to_datetime(training_indice_sp_vol.index)\n",
    "\n",
    "training_ftse, training_indice_sp_vol = training_ftse.align(training_indice_sp_vol, join=\"inner\", axis=0)\n",
    "training_ftse, training_indice_ftse = training_ftse.align(training_indice_ftse, join=\"inner\",axis=0)\n",
    "\n",
    "tickers = sorted(set(training_ftse.columns.get_level_values(1)))\n",
    "for ticker in tickers:\n",
    "    training_ftse[(\"log_return\",ticker)] = np.log(training_ftse[(\"Close\",ticker)]/\n",
    "                                                training_ftse[(\"Open\",ticker)])\n",
    "for ticker in tickers:\n",
    "    training_ftse[(\"open_gap_up\",ticker)] = np.where(\n",
    "        training_ftse[(\"Open\",ticker)].shift(-1)>training_ftse[(\"Close\",ticker)]*1.01,1,0\n",
    "        )\n",
    "for ticker in tickers:\n",
    "    training_ftse[(\"open_gap_down\",ticker)] = np.where(\n",
    "        training_ftse[(\"Open\",ticker)].shift(-1)<training_ftse[(\"Close\",ticker)]*0.99,1,0\n",
    "        )\n",
    "training_ftse = training_ftse.iloc[:,-9:]\n",
    "\n",
    "training_indice_sp_vol[(\"log_return\",\"^VIX\")] = np.log(training_indice_sp_vol[(\"Close\",\"^VIX\")]/\n",
    "                                                training_indice_sp_vol[(\"Open\",\"^VIX\")])\n",
    "training_indice_sp_vol = training_indice_sp_vol.iloc[:,-1]\n",
    "\n",
    "x_classifier_ftse = training_ftse.merge(training_indice_sp_vol,how=\"inner\",left_index=True,right_index=True)\n",
    "previous_indice_day = {}\n",
    "for i in range(0,5):\n",
    "    previous_indice_day[(\"shift\",f\"shift {str(i)}\")] = np.log(\n",
    "        training_indice_ftse[(\"Close\",\"^FTSE\")].shift(i)/training_indice_ftse[(\"Open\",\"^FTSE\")].shift(i)\n",
    "        )\n",
    "previous_indice_day[(\"gap\",\"open_gap_up\")] = np.where(\n",
    "    training_indice_ftse[(\"Open\",\"^FTSE\")].shift(-1)>training_indice_ftse[(\"Close\",\"^FTSE\")]*1.01,1,0\n",
    "    )\n",
    "previous_indice_day[(\"gap\",\"open_gap_down\")] = np.where(\n",
    "    training_indice_ftse[(\"Open\",\"^FTSE\")].shift(-1)<training_indice_ftse[(\"Close\",\"^FTSE\")]*0.99,1,0\n",
    "    )\n",
    "previous_indice_day = pd.DataFrame(previous_indice_day, index=training_indice_ftse.index)\n",
    "x_classifier_ftse_csv = x_classifier_ftse.merge(previous_indice_day,how=\"inner\",left_index=True,right_index=True)\n",
    "x_classifier_ftse_csv.to_csv(\"data/training_data_x_classifier/x_classifier_ftse.csv\")\n",
    "previous_indice_day = previous_indice_day.iloc[i:-2,:]\n",
    "x_classifier_ftse = x_classifier_ftse.iloc[i:-2,:]\n",
    "x_classifier_ftse = x_classifier_ftse.to_numpy()\n",
    "previous_indice_day = previous_indice_day.to_numpy()\n",
    "x_classifier_ftse = np.concatenate((x_classifier_ftse,previous_indice_day), axis=1)\n",
    "y_classifier_ftse = np.where(training_indice_ftse[(\"Close\", \"^FTSE\")].shift(-1)>\n",
    "training_indice_ftse[(\"Open\", \"^FTSE\")].shift(-1),1,0)\n",
    "y_classifier_ftse = y_classifier_ftse[i:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bba72bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision du modèle standard est de 60.698 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55       835\n",
      "           1       0.62      0.69      0.65       941\n",
      "\n",
      "    accuracy                           0.61      1776\n",
      "   macro avg       0.61      0.60      0.60      1776\n",
      "weighted avg       0.61      0.61      0.60      1776\n",
      "\n",
      "La précision du modèle balanced est de 61.092 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.55      0.57       835\n",
      "           1       0.63      0.66      0.64       941\n",
      "\n",
      "    accuracy                           0.61      1776\n",
      "   macro avg       0.61      0.61      0.61      1776\n",
      "weighted avg       0.61      0.61      0.61      1776\n",
      "\n",
      "La précision du modèle signal est de 54.561 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.83      0.63       835\n",
      "           1       0.66      0.29      0.40       941\n",
      "\n",
      "    accuracy                           0.55      1776\n",
      "   macro avg       0.59      0.56      0.52      1776\n",
      "weighted avg       0.59      0.55      0.51      1776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_ftse = False\n",
    "if trained_ftse == False:\n",
    "    class_weight_dict = {   \"standard\":None,\n",
    "                            \"balanced\":\"balanced\",\n",
    "                            \"signal\":{0:1.7,1:1.0}  }\n",
    "    classifier_model_ftse = {}\n",
    "    model_accuracy_ftse = {}\n",
    "    classification_report_ftse = {}\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_classifier_ftse, y_classifier_ftse, test_size=0.3, shuffle=False)\n",
    "    for name,weight in class_weight_dict.items():\n",
    "        model = RandomForestClassifier( n_estimators=4000,\n",
    "                                        max_depth=10,\n",
    "                                        min_samples_split=10,\n",
    "                                        min_samples_leaf=4,\n",
    "                                        max_features='sqrt',\n",
    "                                        class_weight=weight,\n",
    "                                        n_jobs=-1   )\n",
    "        model = model.fit(x_train,y_train)\n",
    "        classifier_model_ftse[name] = model\n",
    "        y_pred = classifier_model_ftse[name].predict(x_test)\n",
    "        model_accuracy_ftse[name] = accuracy_score(y_test, y_pred)\n",
    "        print(f\"La précision du modèle {name} est de {round(model_accuracy_ftse[name]*100,3)} %\")\n",
    "        classification_report_ftse[name] = classification_report(y_test, y_pred)\n",
    "        print(classification_report_ftse[name])\n",
    "    joblib.dump(classifier_model_ftse, \"data/model/classifier_model_ftse.joblib\")\n",
    "    joblib.dump(model_accuracy_ftse, \"data/model/model_accuracy_ftse.joblib\")\n",
    "    joblib.dump(classification_report_ftse, \"data/model/classification_report_ftse.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
